"""
è‡ªå‹•è©•ä¾¡ãƒ©ãƒ³ãƒŠãƒ¼
eval/job*.txt ã¨ eval/resume.txt ã‚’èª­ã¿ã€F1ã€œF4ã‚’å®Ÿè¡Œã—ã¦çµæœJSONã‚’ä¿å­˜
"""
import os
import json
import glob
from datetime import datetime
from typing import Dict, Any
from pathlib import Path

from f1_extract_requirements import extract_requirements
from f2_extract_evidence import extract_evidence
from f3_score import calculate_scores
from f4_generate_improvements import generate_improvements
from llm_judge import evaluate_with_llm_judge, summarize_judge_results
from models import Requirement, Evidence, RequirementWithEvidence, Gap, Improvements


def run_evaluation(
    job_file: str,
    resume_file: str,
    output_file: str,
    options: Dict[str, Any] = None
) -> Dict[str, Any]:
    """
    1ã¤ã®æ±‚äººç¥¨ã«å¯¾ã—ã¦è©•ä¾¡ã‚’å®Ÿè¡Œ
    
    Args:
        job_file: æ±‚äººç¥¨ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        resume_file: è·å‹™çµŒæ­´æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        output_file: å‡ºåŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        options: ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¾æ›¸ï¼ˆLLMè¨­å®šãªã©ï¼‰
        
    Returns:
        Dict[str, Any]: è©•ä¾¡çµæœ
    """
    print(f"\n{'='*60}")
    print(f"è©•ä¾¡å®Ÿè¡Œ: {os.path.basename(job_file)}")
    print(f"{'='*60}")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    with open(job_file, 'r', encoding='utf-8') as f:
        job_text = f.read()
    
    with open(resume_file, 'r', encoding='utf-8') as f:
        resume_text = f.read()
    
    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    if options is None:
        options = {
            "llm_provider": "openai",
            "model_name": None,
            "max_must": 10,
            "max_want": 10,
            "strict_mode": False,
            "verify_quotes": True,
            "max_gaps": 5
        }
    
    result = {
        "timestamp": datetime.now().isoformat(),
        "job_file": os.path.basename(job_file),
        "resume_file": os.path.basename(resume_file),
        "options": options,
        "execution": {}
    }
    
    try:
        # F1: æ±‚äººè¦ä»¶æŠ½å‡º
        print("\n[F1] æ±‚äººè¦ä»¶æŠ½å‡ºä¸­...")
        requirements = extract_requirements(job_text, options)
        result["execution"]["f1"] = {
            "status": "success",
            "requirement_count": len(requirements),
            "requirements": [req.model_dump() for req in requirements]
        }
        print(f"âœ… F1å®Œäº†: {len(requirements)}ä»¶ã®è¦ä»¶ã‚’æŠ½å‡º")
        
        # F2: æ ¹æ‹ æŠ½å‡º
        print("\n[F2] æ ¹æ‹ æŠ½å‡ºä¸­...")
        evidence_map = extract_evidence(resume_text, requirements, options)
        result["execution"]["f2"] = {
            "status": "success",
            "evidence_count": len(evidence_map),
            "evidence": {req_id: ev.model_dump() for req_id, ev in evidence_map.items()}
        }
        print(f"âœ… F2å®Œäº†: {len(evidence_map)}ä»¶ã®æ ¹æ‹ ã‚’åˆ†æ")
        
        # F3: ã‚¹ã‚³ã‚¢è¨ˆç®—
        print("\n[F3] ã‚¹ã‚³ã‚¢è¨ˆç®—ä¸­...")
        score_total, score_must, score_want, matched, gaps, summary = calculate_scores(
            requirements, evidence_map
        )
        result["execution"]["f3"] = {
            "status": "success",
            "score_total": score_total,
            "score_must": score_must,
            "score_want": score_want,
            "matched_count": len(matched),
            "gap_count": len(gaps),
            "summary": summary,
            "matched": [
                {
                    "requirement": m.requirement.model_dump(),
                    "evidence": m.evidence.model_dump()
                }
                for m in matched
            ],
            "gaps": [
                {
                    "requirement": g.requirement.model_dump(),
                    "evidence": g.evidence.model_dump()
                }
                for g in gaps
            ]
        }
        print(f"âœ… F3å®Œäº†: ç·åˆã‚¹ã‚³ã‚¢ {score_total}ç‚¹ (Must: {score_must}, Want: {score_want})")
        
        # F4: æ”¹å–„æ¡ˆç”Ÿæˆ
        print("\n[F4] æ”¹å–„æ¡ˆç”Ÿæˆä¸­...")
        improvements = generate_improvements(
            job_text, resume_text, requirements, matched, gaps, options
        )
        result["execution"]["f4"] = {
            "status": "success",
            "improvements": improvements.model_dump()
        }
        print(f"âœ… F4å®Œäº†: {len(improvements.action_items)}ä»¶ã®è¡Œå‹•è¨ˆç”»ã‚’ç”Ÿæˆ")
        
        # LLM-as-Judge: è©•ä¾¡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if options.get("enable_judge", False):
            print("\n[LLM-as-Judge] è©•ä¾¡å®Ÿè¡Œä¸­...")
            try:
                judge_output = evaluate_with_llm_judge(
                    job_text, resume_text, requirements, evidence_map,
                    matched, gaps, improvements, options
                )
                judge_summary = summarize_judge_results(judge_output)
                result["execution"]["judge"] = {
                    "status": "success",
                    "judge_output": judge_output.model_dump(),
                    "summary": judge_summary
                }
                print(f"âœ… LLM-as-Judgeå®Œäº†: ç·åˆã‚¹ã‚³ã‚¢ {judge_output.overall_score:.2f}")
            except Exception as judge_error:
                result["execution"]["judge"] = {
                    "status": "error",
                    "error": str(judge_error)
                }
                print(f"âš ï¸  LLM-as-Judgeã‚¨ãƒ©ãƒ¼ï¼ˆç„¡è¦–ï¼‰: {judge_error}")
        
        result["status"] = "success"
        print(f"\nâœ… è©•ä¾¡å®Œäº†: {os.path.basename(output_file)}")
        
    except Exception as e:
        result["status"] = "error"
        result["error"] = str(e)
        result["error_type"] = type(e).__name__
        import traceback
        result["traceback"] = traceback.format_exc()
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        print(traceback.format_exc())
    
    # JSONä¿å­˜
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    return result


def run_all_evaluations(
    eval_dir: str = "eval",
    output_dir: str = "eval/outputs",
    options: Dict[str, Any] = None
):
    """
    å…¨ã¦ã®æ±‚äººç¥¨ã«å¯¾ã—ã¦è©•ä¾¡ã‚’å®Ÿè¡Œ
    
    Args:
        eval_dir: evalãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        options: ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¾æ›¸
    """
    # æ±‚äººç¥¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    job_files = sorted(glob.glob(os.path.join(eval_dir, "job*.txt")))
    resume_file = os.path.join(eval_dir, "resume.txt")
    
    if not job_files:
        print(f"âŒ æ±‚äººç¥¨ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {eval_dir}/job*.txt")
        return
    
    if not os.path.exists(resume_file):
        print(f"âŒ è·å‹™çµŒæ­´æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {resume_file}")
        return
    
    print(f"ğŸ“‹ è©•ä¾¡å¯¾è±¡: {len(job_files)}ä»¶ã®æ±‚äººç¥¨")
    print(f"ğŸ“„ è·å‹™çµŒæ­´æ›¸: {resume_file}")
    print(f"ğŸ’¾ å‡ºåŠ›å…ˆ: {output_dir}")
    
    results = []
    
    for job_file in job_files:
        # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ±ºå®šï¼ˆjob1.txt -> job1.jsonï¼‰
        job_basename = os.path.basename(job_file)
        job_name = os.path.splitext(job_basename)[0]  # job1
        output_file = os.path.join(output_dir, f"{job_name}.json")
        
        # è©•ä¾¡å®Ÿè¡Œ
        result = run_evaluation(job_file, resume_file, output_file, options)
        results.append(result)
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print(f"\n{'='*60}")
    print("ğŸ“Š è©•ä¾¡ã‚µãƒãƒªãƒ¼")
    print(f"{'='*60}")
    
    success_count = sum(1 for r in results if r.get("status") == "success")
    error_count = len(results) - success_count
    
    print(f"æˆåŠŸ: {success_count}ä»¶ / ã‚¨ãƒ©ãƒ¼: {error_count}ä»¶")
    
    if success_count > 0:
        print("\nã‚¹ã‚³ã‚¢ä¸€è¦§:")
        for result in results:
            if result.get("status") == "success":
                job_name = result.get("job_file", "unknown")
                f3 = result.get("execution", {}).get("f3", {})
                score_total = f3.get("score_total", "N/A")
                judge_info = ""
                if result.get("execution", {}).get("judge", {}).get("status") == "success":
                    judge_summary = result.get("execution", {}).get("judge", {}).get("summary", {})
                    judge_score = judge_summary.get("overall_score", "N/A")
                    if isinstance(judge_score, (int, float)):
                        judge_info = f" (Judge: {judge_score:.2f})"
                print(f"  {job_name}: {score_total}ç‚¹{judge_info}")
    
    print(f"\nğŸ’¾ çµæœã¯ {output_dir} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="è‡ªå‹•è©•ä¾¡ãƒ©ãƒ³ãƒŠãƒ¼")
    parser.add_argument(
        "--eval-dir",
        type=str,
        default="eval",
        help="evalãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: evalï¼‰"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="eval/outputs",
        help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: eval/outputsï¼‰"
    )
    parser.add_argument(
        "--llm-provider",
        type=str,
        choices=["openai", "anthropic"],
        default="openai",
        help="LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: openaiï¼‰"
    )
    parser.add_argument(
        "--model-name",
        type=str,
        default=None,
        help="ãƒ¢ãƒ‡ãƒ«åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰"
    )
    parser.add_argument(
        "--max-must",
        type=int,
        default=10,
        help="Mustè¦ä»¶ã®æœ€å¤§ä»¶æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ï¼‰"
    )
    parser.add_argument(
        "--max-want",
        type=int,
        default=10,
        help="Wantè¦ä»¶ã®æœ€å¤§ä»¶æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ï¼‰"
    )
    parser.add_argument(
        "--strict-mode",
        action="store_true",
        help="Strictãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–"
    )
    parser.add_argument(
        "--no-verify-quotes",
        action="store_true",
        help="å¼•ç”¨æ¤œè¨¼ã‚’ç„¡åŠ¹åŒ–"
    )
    parser.add_argument(
        "--max-gaps",
        type=int,
        default=5,
        help="æ”¹å–„æ¡ˆç”Ÿæˆæ™‚ã®æœ€å¤§ã‚®ãƒ£ãƒƒãƒ—ä»¶æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ï¼‰"
    )
    parser.add_argument(
        "--enable-judge",
        action="store_true",
        help="LLM-as-Judgeè©•ä¾¡ã‚’æœ‰åŠ¹åŒ–"
    )
    parser.add_argument(
        "--judge-temperature",
        type=float,
        default=0.0,
        help="LLM-as-Judgeã®Temperatureï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.0ï¼‰"
    )
    
    args = parser.parse_args()
    
    options = {
        "llm_provider": args.llm_provider,
        "model_name": args.model_name,
        "max_must": args.max_must,
        "max_want": args.max_want,
        "strict_mode": args.strict_mode,
        "verify_quotes": not args.no_verify_quotes,
        "max_gaps": args.max_gaps,
        "enable_judge": args.enable_judge,
        "judge_temperature": args.judge_temperature
    }
    
    run_all_evaluations(
        eval_dir=args.eval_dir,
        output_dir=args.output_dir,
        options=options
    )

