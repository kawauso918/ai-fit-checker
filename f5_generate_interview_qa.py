"""
F5: é¢æ¥æƒ³å®šQ&Aã‚’ç”Ÿæˆ
åˆ†æçµæœã‹ã‚‰é¢æ¥ã§èã‹ã‚Œãã†ãªè³ªå•ã¨å›ç­”ã®éª¨å­ã‚’ç”Ÿæˆ
"""
import os
from typing import List, Optional
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import PromptTemplate

from models import (
    RequirementWithEvidence,
    Gap,
    InterviewQA,
    InterviewQAs,
    F5Output
)

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()


def generate_interview_qa(
    job_text: str,
    resume_text: str,
    matched: List[RequirementWithEvidence],
    gaps: List[Gap],
    summary: str,
    options: Optional[dict] = None
) -> InterviewQAs:
    """
    åˆ†æçµæœã‹ã‚‰é¢æ¥æƒ³å®šQ&Aã‚’ç”Ÿæˆã™ã‚‹ï¼ˆF5ï¼‰

    Args:
        job_text: æ±‚äººç¥¨ã®ãƒ†ã‚­ã‚¹ãƒˆ
        resume_text: è·å‹™çµŒæ­´æ›¸ã®ãƒ†ã‚­ã‚¹ãƒˆ
        matched: ãƒãƒƒãƒã—ãŸè¦ä»¶ã¨æ ¹æ‹ ã®ãƒšã‚¢
        gaps: ã‚®ãƒ£ãƒƒãƒ—ã®ã‚ã‚‹è¦ä»¶
        summary: ã‚¹ã‚³ã‚¢ã®ç·è©•
        options: ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¾æ›¸
            - llm_provider: "openai" or "anthropic"ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ "openai"ï¼‰
            - model_name: ãƒ¢ãƒ‡ãƒ«åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ gpt-4o-miniï¼‰

    Returns:
        InterviewQAs: é¢æ¥Q&A
    """
    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    if options is None:
        options = {}

    llm_provider = options.get("llm_provider", "openai")
    model_name = options.get("model_name", None)
    
    # ã‚³ã‚¹ãƒˆé‡è¦–ã®ãŸã‚ã€miniãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«
    if not model_name:
        if llm_provider == "anthropic":
            model_name = "claude-3-haiku-20240307"  # ã‚ˆã‚Šå®‰ä¾¡ãªãƒ¢ãƒ‡ãƒ«
        else:
            model_name = "gpt-4o-mini"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

    # LLMã®åˆæœŸåŒ–
    try:
        if llm_provider == "anthropic":
            llm = ChatAnthropic(
                model=model_name,
                temperature=0.3,  # å°‘ã—å‰µé€ æ€§ã‚’æŒãŸã›ã‚‹
                api_key=os.getenv("ANTHROPIC_API_KEY")
            )
        else:  # openai
            llm = ChatOpenAI(
                model=model_name,
                temperature=0.3,
                api_key=os.getenv("OPENAI_API_KEY")
            )

        # ãƒ‘ãƒ¼ã‚µãƒ¼è¨­å®š
        parser = PydanticOutputParser(pydantic_object=F5Output)

        # ãƒãƒƒãƒæƒ…å ±ã‚’æ–‡å­—åˆ—åŒ–
        matched_str = "\n".join([
            f"- {m.requirement.description} (ä¸€è‡´åº¦: {m.evidence.confidence:.0%})"
            for m in matched[:5]  # æœ€å¤§5ä»¶
        ])

        # ã‚®ãƒ£ãƒƒãƒ—æƒ…å ±ã‚’æ–‡å­—åˆ—åŒ–
        gaps_str = "\n".join([
            f"- {g.requirement.description} ({g.requirement.category.value}): {g.evidence.reason}"
            for g in gaps[:5]  # æœ€å¤§5ä»¶
        ])

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        prompt_template = PromptTemplate(
            template="""ã‚ãªãŸã¯é¢æ¥å®˜ã§ã™ã€‚ä»¥ä¸‹ã®åˆ†æçµæœã‚’ã‚‚ã¨ã«ã€é¢æ¥ã§èã‹ã‚Œãã†ãªè³ªå•ã‚’10å•ç¨‹åº¦ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

æ±‚äººç¥¨ï¼ˆæŠœç²‹ï¼‰ï¼š
{job_text}

è·å‹™çµŒæ­´æ›¸ï¼ˆæŠœç²‹ï¼‰ï¼š
{resume_text}

åˆ†æçµæœã‚µãƒãƒªãƒ¼ï¼š
{summary}

ãƒãƒƒãƒã—ãŸè¦ä»¶ï¼ˆå¼·ã¿ï¼‰ï¼š
{matched_str}

ã‚®ãƒ£ãƒƒãƒ—ã®ã‚ã‚‹è¦ä»¶ï¼ˆä¸è¶³ç‚¹ï¼‰ï¼š
{gaps_str}

è³ªå•ç”Ÿæˆãƒ«ãƒ¼ãƒ«ï¼š
1. **è³ªå•ã®ç¨®é¡**ï¼š
   - å¼·ã¿æ·±æ˜ã‚Šï¼ˆ3-4å•ï¼‰ï¼šãƒãƒƒãƒã—ãŸè¦ä»¶ã«ã¤ã„ã¦ã€å…·ä½“çš„ãªçµŒé¨“ã‚„æˆæœã‚’èãè³ªå•
   - ã‚®ãƒ£ãƒƒãƒ—çªã£è¾¼ã¿ï¼ˆ3-4å•ï¼‰ï¼šä¸è¶³ã—ã¦ã„ã‚‹è¦ä»¶ã«ã¤ã„ã¦ã€ã©ã†å¯¾å¿œã™ã‚‹ã‹èãè³ªå•
   - å¿—æœ›å‹•æ©Ÿå¯„ã›ï¼ˆ2-3å•ï¼‰ï¼šæ±‚äººç¥¨ã¨è·å‹™çµŒæ­´ã®é–¢é€£æ€§ã€å¿—æœ›å‹•æ©Ÿã‚’èãè³ªå•

2. **å›ç­”ã®éª¨å­ï¼ˆanswer_outlineï¼‰**ï¼š
   - è·å‹™çµŒæ­´ã«è¨˜è¼‰ãŒã‚ã‚‹å†…å®¹ï¼šå…·ä½“çš„ãªçµŒé¨“ãƒ»æˆæœãƒ»æ•°å€¤ã‚’å«ã‚ã‚‹
   - è·å‹™çµŒæ­´ã«è¨˜è¼‰ãŒãªã„å†…å®¹ï¼šã€Œå­¦ç¿’ä¸­ã€ã€Œè¨ˆç”»ä¸­ã€ã€Œä»Šå¾Œå–ã‚Šçµ„ã¿ãŸã„ã€ãªã©ç¾å®Ÿçš„ãªè¡¨ç¾ã‚’ä½¿ã†
   - æé€ ã¯çµ¶å¯¾ã«ç¦æ­¢ï¼šè·å‹™çµŒæ­´ã«ãªã„çµŒé¨“ã‚’ã€ŒçµŒé¨“ãŒã‚ã‚‹ã€ã¨æ›¸ã‹ãªã„
   - ç®‡æ¡æ›¸ãã§3-5é …ç›®ç¨‹åº¦

3. **è³ªå•ã®å…·ä½“æ€§**ï¼š
   - æŠ½è±¡çš„ã™ããšã€å…·ä½“çš„ãªçµŒé¨“ã‚„è¡Œå‹•ã‚’èãè³ªå•
   - ã€Œãªãœã€ã€Œã©ã®ã‚ˆã†ã«ã€ã€Œã©ã®ã‚ˆã†ãªæˆæœã€ãªã©æ·±æ˜ã‚Šã™ã‚‹è³ªå•

**é‡è¦**: è·å‹™çµŒæ­´ã«ãªã„å†…å®¹ã¯ã€Œå­¦ç¿’ä¸­/è¨ˆç”»ã€ã¨ã—ã¦å›ç­”éª¨å­ã‚’ä½œã‚‹ã“ã¨ã€‚æé€ ã¯çµ¶å¯¾ã«ç¦æ­¢ã€‚

{format_instructions}
""",
            input_variables=["job_text", "resume_text", "summary", "matched_str", "gaps_str"],
            partial_variables={"format_instructions": parser.get_format_instructions()}
        )

        # ãƒ†ã‚­ã‚¹ãƒˆã‚’å¿…è¦æœ€å°é™ã«ã‚«ãƒƒãƒˆ
        job_text_trimmed = job_text[:1000] if len(job_text) > 1000 else job_text
        resume_text_trimmed = resume_text[:1000] if len(resume_text) > 1000 else resume_text

        # LLMå®Ÿè¡Œã¨ãƒ‘ãƒ¼ã‚¹ï¼ˆæœ€å¤§3å›ãƒªãƒˆãƒ©ã‚¤ï¼‰
        max_retries = 3
        for attempt in range(max_retries):
            try:
                prompt = prompt_template.format(
                    job_text=job_text_trimmed,
                    resume_text=resume_text_trimmed,
                    summary=summary,
                    matched_str=matched_str,
                    gaps_str=gaps_str
                )
                output = llm.invoke(prompt)
                result = parser.parse(output.content)
                interview_qas = result.interview_qas
                break
            except Exception as parse_error:
                if attempt == max_retries - 1:
                    # æœ€å¾Œã®è©¦è¡Œã§ã‚‚å¤±æ•—ã—ãŸå ´åˆã¯ä¾‹å¤–ã‚’æŠ•ã’ã‚‹
                    raise parse_error
                # ãƒªãƒˆãƒ©ã‚¤

    except Exception as e:
        print(f"âš ï¸  LLMç”Ÿæˆã«å¤±æ•—ã€fallbackã‚’ä½¿ç”¨: {e}")
        # Fallback: ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ç”Ÿæˆ
        interview_qas = _fallback_generate(matched, gaps)

    return interview_qas


def _fallback_generate(
    matched: List[RequirementWithEvidence],
    gaps: List[Gap]
) -> InterviewQAs:
    """
    Fallback: ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§ç°¡æ˜“çš„ãªQ&Aã‚’ç”Ÿæˆ

    Args:
        matched: ãƒãƒƒãƒã—ãŸè¦ä»¶
        gaps: ã‚®ãƒ£ãƒƒãƒ—ã®ã‚ã‚‹è¦ä»¶

    Returns:
        InterviewQAs: é¢æ¥Q&A
    """
    qa_list = []

    # å¼·ã¿æ·±æ˜ã‚Šï¼ˆæœ€å¤§3å•ï¼‰
    for i, m in enumerate(matched[:3], 1):
        qa_list.append(InterviewQA(
            question=f"{m.requirement.description}ã«ã¤ã„ã¦ã€å…·ä½“çš„ãªçµŒé¨“ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚",
            answer_outline=[
                "è·å‹™çµŒæ­´æ›¸ã«è¨˜è¼‰ã—ãŸçµŒé¨“ã‚’å…·ä½“çš„ã«èª¬æ˜",
                "ä½¿ç”¨ã—ãŸæŠ€è¡“ã‚„ãƒ„ãƒ¼ãƒ«",
                "é”æˆã—ãŸæˆæœã‚„æ•°å€¤"
            ]
        ))

    # ã‚®ãƒ£ãƒƒãƒ—çªã£è¾¼ã¿ï¼ˆæœ€å¤§3å•ï¼‰
    for i, g in enumerate(gaps[:3], 1):
        qa_list.append(InterviewQA(
            question=f"{g.requirement.description}ã«ã¤ã„ã¦ã€ã©ã®ã‚ˆã†ã«å¯¾å¿œã—ã¾ã™ã‹ï¼Ÿ",
            answer_outline=[
                "ç¾çŠ¶ã®ç†è§£",
                "å­¦ç¿’è¨ˆç”»ã‚„å–ã‚Šçµ„ã¿æ–¹é‡",
                "ä»Šå¾Œã®ç›®æ¨™"
            ]
        ))

    # å¿—æœ›å‹•æ©Ÿå¯„ã›ï¼ˆ2å•ï¼‰
    qa_list.append(InterviewQA(
        question="ã“ã®æ±‚äººã«å¿œå‹Ÿã—ãŸç†ç”±ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚",
        answer_outline=[
            "æ±‚äººç¥¨ã®ã©ã®ç‚¹ã«é­…åŠ›ã‚’æ„Ÿã˜ãŸã‹",
            "è‡ªåˆ†ã®çµŒé¨“ã‚„ã‚¹ã‚­ãƒ«ã¨ã®é–¢é€£æ€§",
            "ä»Šå¾Œã®ã‚­ãƒ£ãƒªã‚¢ãƒ—ãƒ©ãƒ³"
        ]
    ))

    qa_list.append(InterviewQA(
        question="å½“ç¤¾ã§ã©ã®ã‚ˆã†ãªè²¢çŒ®ãŒã§ãã¾ã™ã‹ï¼Ÿ",
        answer_outline=[
            "å¼·ã¿ã‚’æ´»ã‹ã›ã‚‹é ˜åŸŸ",
            "å…·ä½“çš„ãªè²¢çŒ®å†…å®¹",
            "ãƒãƒ¼ãƒ ã¸ã®ä¾¡å€¤æä¾›"
        ]
    ))

    return InterviewQAs(qa_list=qa_list[:10])  # æœ€å¤§10å•


# ==================== ãƒ†ã‚¹ãƒˆç”¨ã‚³ãƒ¼ãƒ‰ ====================
if __name__ == "__main__":
    from models import Requirement, Evidence, RequirementType, ConfidenceLevel

    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
    matched = [
        RequirementWithEvidence(
            requirement=Requirement(
                req_id="REQ_001",
                category=RequirementType.MUST,
                description="Pythoné–‹ç™ºçµŒé¨“3å¹´ä»¥ä¸Š",
                importance=5,
                job_quote="Pythoné–‹ç™ºçµŒé¨“3å¹´ä»¥ä¸Š",
                weight=1.0
            ),
            evidence=Evidence(
                req_id="REQ_001",
                resume_quotes=["Pythonã‚’ä½¿ç”¨ã—ãŸWebã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºã«5å¹´é–“å¾“äº‹"],
                confidence=1.0,
                confidence_level=ConfidenceLevel.HIGH,
                reason="5å¹´é–“ã®PythonçµŒé¨“ãŒæ˜è¨˜ã•ã‚Œã¦ã„ã‚‹"
            )
        )
    ]

    gaps = [
        Gap(
            requirement=Requirement(
                req_id="REQ_002",
                category=RequirementType.MUST,
                description="Docker/Kubernetesã®å®Ÿå‹™çµŒé¨“",
                importance=4,
                job_quote="Docker/Kubernetesã®å®Ÿå‹™çµŒé¨“",
                weight=1.0
            ),
            evidence=Evidence(
                req_id="REQ_002",
                resume_quotes=[],
                confidence=0.0,
                confidence_level=ConfidenceLevel.NONE,
                reason="Docker/Kubernetesã«é–¢ã™ã‚‹è¨˜è¼‰ãŒãªã„"
            )
        )
    ]

    sample_job_text = "Pythoné–‹ç™ºçµŒé¨“3å¹´ä»¥ä¸Šã€Docker/Kubernetesã®å®Ÿå‹™çµŒé¨“"
    sample_resume_text = "Pythonã‚’ä½¿ç”¨ã—ãŸWebã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºã«5å¹´é–“å¾“äº‹"
    sample_summary = "ç·åˆé©åˆåº¦ã¯ä¸­ç¨‹åº¦ã§ã™ï¼ˆ50ç‚¹ï¼‰ã€‚Mustè¦ä»¶ã®ã†ã¡1ä»¶ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚"

    try:
        interview_qas = generate_interview_qa(
            job_text=sample_job_text,
            resume_text=sample_resume_text,
            matched=matched,
            gaps=gaps,
            summary=sample_summary,
            options={"llm_provider": "openai"}
        )

        print(f"\n{'='*60}")
        print("ğŸ“‹ é¢æ¥æƒ³å®šQ&A")
        print(f"{'='*60}")

        for i, qa in enumerate(interview_qas.qa_list, 1):
            print(f"\n{i}. {qa.question}")
            print("   å›ç­”ã®éª¨å­:")
            for outline in qa.answer_outline:
                print(f"   - {outline}")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()













