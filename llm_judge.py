"""
LLM-as-Judge: è©•ä¾¡æ©Ÿèƒ½
å¼•ç”¨ã®æ­£ç¢ºæ€§ã€Must/Wantåˆ†é¡ã®å¦¥å½“æ€§ã€ã‚®ãƒ£ãƒƒãƒ—ã®å¦¥å½“æ€§ã€æ”¹å–„æ¡ˆã®å…·ä½“æ€§ã‚’è©•ä¾¡
"""
import os
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import PromptTemplate
from pydantic import BaseModel, Field

from models import Requirement, Evidence, RequirementWithEvidence, Gap, Improvements

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()


# ==================== è©•ä¾¡çµæœãƒ¢ãƒ‡ãƒ« ====================
class QuoteAccuracyEvaluation(BaseModel):
    """å¼•ç”¨ã®æ­£ç¢ºæ€§è©•ä¾¡"""
    req_id: str = Field(..., description="è¦ä»¶ID")
    quote: str = Field(..., description="è©•ä¾¡å¯¾è±¡ã®å¼•ç”¨")
    is_accurate: bool = Field(..., description="æ­£ç¢ºã‹ã©ã†ã‹")
    accuracy_score: float = Field(..., ge=0.0, le=1.0, description="æ­£ç¢ºæ€§ã‚¹ã‚³ã‚¢ï¼ˆ0.0ã€œ1.0ï¼‰")
    reason: str = Field(..., description="è©•ä¾¡ç†ç”±")
    suggestion: Optional[str] = Field(None, description="æ”¹å–„ææ¡ˆï¼ˆã‚ã‚Œã°ï¼‰")


class ClassificationEvaluation(BaseModel):
    """Must/Wantåˆ†é¡ã®å¦¥å½“æ€§è©•ä¾¡"""
    req_id: str = Field(..., description="è¦ä»¶ID")
    current_category: str = Field(..., description="ç¾åœ¨ã®åˆ†é¡ï¼ˆMust/Wantï¼‰")
    is_correct: bool = Field(..., description="åˆ†é¡ãŒæ­£ã—ã„ã‹")
    correct_category: Optional[str] = Field(None, description="æ­£ã—ã„åˆ†é¡ï¼ˆé–“é•ã£ã¦ã„ã‚‹å ´åˆï¼‰")
    confidence_score: float = Field(..., ge=0.0, le=1.0, description="åˆ†é¡ã®ä¿¡é ¼åº¦ï¼ˆ0.0ã€œ1.0ï¼‰")
    reason: str = Field(..., description="è©•ä¾¡ç†ç”±")


class GapEvaluation(BaseModel):
    """ã‚®ãƒ£ãƒƒãƒ—ã®å¦¥å½“æ€§è©•ä¾¡"""
    req_id: str = Field(..., description="è¦ä»¶ID")
    is_gap_correct: bool = Field(..., description="ã‚®ãƒ£ãƒƒãƒ—åˆ¤å®šãŒæ­£ã—ã„ã‹")
    gap_score: float = Field(..., ge=0.0, le=1.0, description="ã‚®ãƒ£ãƒƒãƒ—åˆ¤å®šã®å¦¥å½“æ€§ã‚¹ã‚³ã‚¢ï¼ˆ0.0ã€œ1.0ï¼‰")
    reason: str = Field(..., description="è©•ä¾¡ç†ç”±")
    suggestion: Optional[str] = Field(None, description="æ”¹å–„ææ¡ˆï¼ˆã‚ã‚Œã°ï¼‰")


class ImprovementSpecificityEvaluation(BaseModel):
    """æ”¹å–„æ¡ˆã®å…·ä½“æ€§è©•ä¾¡"""
    improvement_type: str = Field(..., description="æ”¹å–„æ¡ˆã®ç¨®é¡ï¼ˆresume_edit/action_itemï¼‰")
    target: str = Field(..., description="å¯¾è±¡ï¼ˆgap_idã¾ãŸã¯actionå†…å®¹ï¼‰")
    specificity_score: float = Field(..., ge=0.0, le=1.0, description="å…·ä½“æ€§ã‚¹ã‚³ã‚¢ï¼ˆ0.0ã€œ1.0ï¼‰")
    is_specific: bool = Field(..., description="å…·ä½“çš„ã‹ã©ã†ã‹")
    reason: str = Field(..., description="è©•ä¾¡ç†ç”±")
    suggestion: Optional[str] = Field(None, description="æ”¹å–„ææ¡ˆï¼ˆã‚ã‚Œã°ï¼‰")


class JudgeOutput(BaseModel):
    """LLM-as-Judgeã®å‡ºåŠ›"""
    quote_accuracy: List[QuoteAccuracyEvaluation] = Field(
        default_factory=list,
        description="å¼•ç”¨ã®æ­£ç¢ºæ€§è©•ä¾¡ãƒªã‚¹ãƒˆ"
    )
    classification: List[ClassificationEvaluation] = Field(
        default_factory=list,
        description="Must/Wantåˆ†é¡ã®å¦¥å½“æ€§è©•ä¾¡ãƒªã‚¹ãƒˆ"
    )
    gap_validity: List[GapEvaluation] = Field(
        default_factory=list,
        description="ã‚®ãƒ£ãƒƒãƒ—ã®å¦¥å½“æ€§è©•ä¾¡ãƒªã‚¹ãƒˆ"
    )
    improvement_specificity: List[ImprovementSpecificityEvaluation] = Field(
        default_factory=list,
        description="æ”¹å–„æ¡ˆã®å…·ä½“æ€§è©•ä¾¡ãƒªã‚¹ãƒˆ"
    )
    overall_score: float = Field(..., ge=0.0, le=1.0, description="ç·åˆè©•ä¾¡ã‚¹ã‚³ã‚¢ï¼ˆ0.0ã€œ1.0ï¼‰")
    overall_feedback: str = Field(..., description="ç·åˆãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯")


# ==================== è©•ä¾¡é–¢æ•° ====================
def evaluate_with_llm_judge(
    job_text: str,
    resume_text: str,
    requirements: List[Requirement],
    evidence_map: Dict[str, Evidence],
    matched: List[RequirementWithEvidence],
    gaps: List[Gap],
    improvements: Improvements,
    options: Optional[Dict[str, Any]] = None
) -> JudgeOutput:
    """
    LLM-as-Judgeã§è©•ä¾¡ã‚’å®Ÿè¡Œ
    
    Args:
        job_text: æ±‚äººç¥¨ã®ãƒ†ã‚­ã‚¹ãƒˆ
        resume_text: è·å‹™çµŒæ­´æ›¸ã®ãƒ†ã‚­ã‚¹ãƒˆ
        requirements: è¦ä»¶ãƒªã‚¹ãƒˆ
        evidence_map: æ ¹æ‹ ãƒãƒƒãƒ—
        matched: ãƒãƒƒãƒã—ãŸè¦ä»¶
        gaps: ã‚®ãƒ£ãƒƒãƒ—ã®ã‚ã‚‹è¦ä»¶
        improvements: æ”¹å–„æ¡ˆ
        options: ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¾æ›¸
        
    Returns:
        JudgeOutput: è©•ä¾¡çµæœ
    """
    if options is None:
        options = {}
    
    llm_provider = options.get("llm_provider", "openai")
    model_name = options.get("model_name", None)
    judge_temperature = options.get("judge_temperature", 0.0)
    
    # LLMã®åˆæœŸåŒ–
    try:
        if llm_provider == "anthropic":
            llm = ChatAnthropic(
                model=model_name or "claude-3-5-sonnet-20241022",
                temperature=judge_temperature,
                api_key=os.getenv("ANTHROPIC_API_KEY")
            )
        else:  # openai
            llm = ChatOpenAI(
                model=model_name or "gpt-4o-mini",
                temperature=judge_temperature,
                api_key=os.getenv("OPENAI_API_KEY")
            )
    except Exception as e:
        raise Exception(f"LLMåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ãƒ‘ãƒ¼ã‚µãƒ¼è¨­å®š
    parser = PydanticOutputParser(pydantic_object=JudgeOutput)
    
    # è©•ä¾¡å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
    requirements_str = "\n".join([
        f"[{req.req_id}] {req.category.value}: {req.description} (å¼•ç”¨: {req.job_quote[:100]}...)"
        for req in requirements
    ])
    
    # å¼•ç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ï¼ˆè©•ä¾¡ç”¨ï¼‰
    quote_samples = []
    for ev in evidence_map.values():
        if ev.resume_quotes:
            quote_samples.append(f"[{ev.req_id}] {ev.resume_quotes[0][:100]}...")
    quotes_str = "\n".join(quote_samples[:5]) if quote_samples else "å¼•ç”¨ãªã—"
    
    # ã‚®ãƒ£ãƒƒãƒ—ã®ã‚µãƒ³ãƒ—ãƒ«
    gaps_str = "\n".join([
        f"[{g.requirement.req_id}] {g.requirement.description} (ç†ç”±: {g.evidence.reason[:100]}...)"
        for g in gaps[:5]
    ]) if gaps else "ã‚®ãƒ£ãƒƒãƒ—ãªã—"
    
    # æ”¹å–„æ¡ˆã®ã‚µãƒ³ãƒ—ãƒ«
    improvements_str = ""
    if improvements.resume_edits:
        improvements_str += "è·å‹™çµŒæ­´æ›¸ç·¨é›†æ¡ˆ:\n"
        for edit in improvements.resume_edits[:3]:
            improvements_str += f"- {edit.template[:100]}...\n"
    if improvements.action_items:
        improvements_str += "\nè¡Œå‹•è¨ˆç”»:\n"
        for item in improvements.action_items[:3]:
            improvements_str += f"- [{item.priority}] {item.action[:100]}...\n"
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
    prompt_template = PromptTemplate(
        template="""ã‚ãªãŸã¯AIå¿œå‹Ÿé©åˆåº¦ãƒã‚§ãƒƒã‚«ãƒ¼ã®è©•ä¾¡å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®åˆ†æçµæœã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

ã€æ±‚äººç¥¨ã€‘
{job_text}

ã€è·å‹™çµŒæ­´æ›¸ã€‘
{resume_text}

ã€æŠ½å‡ºã•ã‚ŒãŸè¦ä»¶ã€‘
{requirements_str}

ã€å¼•ç”¨ã‚µãƒ³ãƒ—ãƒ«ã€‘
{quotes_str}

ã€ã‚®ãƒ£ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒ«ã€‘
{gaps_str}

ã€æ”¹å–„æ¡ˆã‚µãƒ³ãƒ—ãƒ«ã€‘
{improvements_str}

è©•ä¾¡è¦³ç‚¹ï¼š

1. **å¼•ç”¨ã®æ­£ç¢ºæ€§ï¼ˆQuote Accuracyï¼‰**
   - resume_quotesãŒå®Ÿéš›ã«resume_textå†…ã«å­˜åœ¨ã™ã‚‹ã‹
   - æ”¹å¤‰ãƒ»è¦ç´„ã•ã‚Œã¦ã„ãªã„ã‹
   - æ–‡è„ˆãŒé©åˆ‡ã‹
   - è©•ä¾¡å¯¾è±¡: å„Evidenceã®resume_quotesï¼ˆæœ€å¤§5ä»¶ã¾ã§ï¼‰

2. **Must/Wantåˆ†é¡ã®å¦¥å½“æ€§ï¼ˆClassificationï¼‰**
   - å„è¦ä»¶ãŒé©åˆ‡ãªã‚«ãƒ†ã‚´ãƒªï¼ˆMust/Wantï¼‰ã«åˆ†é¡ã•ã‚Œã¦ã„ã‚‹ã‹
   - æ±‚äººç¥¨ã®æ–‡è„ˆã‹ã‚‰åˆ¤æ–­
   - è©•ä¾¡å¯¾è±¡: å…¨è¦ä»¶

3. **ã‚®ãƒ£ãƒƒãƒ—ã®å¦¥å½“æ€§ï¼ˆGap Validityï¼‰**
   - ã‚®ãƒ£ãƒƒãƒ—åˆ¤å®šãŒé©åˆ‡ã‹ï¼ˆæœ¬å½“ã«ä¸è¶³ã—ã¦ã„ã‚‹ã‹ï¼‰
   - æ ¹æ‹ ï¼ˆevidence.reasonï¼‰ãŒå¦¥å½“ã‹
   - è©•ä¾¡å¯¾è±¡: ã‚®ãƒ£ãƒƒãƒ—ã®ã‚ã‚‹è¦ä»¶ï¼ˆæœ€å¤§5ä»¶ã¾ã§ï¼‰

4. **æ”¹å–„æ¡ˆã®å…·ä½“æ€§ï¼ˆImprovement Specificityï¼‰**
   - æ”¹å–„æ¡ˆãŒã€Œä½•ã‚’ã©ã†æ›¸ã/ä½•ã‚’ã‚„ã‚‹ã€ã¾ã§å…·ä½“çš„ã‹
   - å®Ÿè¡Œå¯èƒ½ã‹
   - è©•ä¾¡å¯¾è±¡: resume_editsã¨action_itemsï¼ˆå„æœ€å¤§3ä»¶ã¾ã§ï¼‰

è©•ä¾¡ãƒ«ãƒ¼ãƒ«ï¼š
- å„è©•ä¾¡é …ç›®ã«å¯¾ã—ã¦ã‚¹ã‚³ã‚¢ï¼ˆ0.0ã€œ1.0ï¼‰ã‚’ä»˜ä¸
- å•é¡ŒãŒã‚ã‚Œã°å…·ä½“çš„ãªç†ç”±ã¨æ”¹å–„ææ¡ˆã‚’è¨˜è¼‰
- ç·åˆè©•ä¾¡ã‚¹ã‚³ã‚¢ã¯å„è¦³ç‚¹ã®å¹³å‡å€¤
- ç·åˆãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¯æ”¹å–„ãŒå¿…è¦ãªç‚¹ã‚’å„ªå…ˆçš„ã«æŒ‡æ‘˜

{format_instructions}
""",
        input_variables=["job_text", "resume_text", "requirements_str", "quotes_str", "gaps_str", "improvements_str"],
        partial_variables={"format_instructions": parser.get_format_instructions()}
    )
    
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚«ãƒƒãƒˆï¼ˆé•·ã™ãã‚‹å ´åˆï¼‰
    job_text_trimmed = job_text[:2000] + "..." if len(job_text) > 2000 else job_text
    resume_text_trimmed = resume_text[:2000] + "..." if len(resume_text) > 2000 else resume_text
    
    # LLMå®Ÿè¡Œ
    try:
        prompt = prompt_template.format(
            job_text=job_text_trimmed,
            resume_text=resume_text_trimmed,
            requirements_str=requirements_str,
            quotes_str=quotes_str,
            gaps_str=gaps_str,
            improvements_str=improvements_str
        )
        output = llm.invoke(prompt)
        result = parser.parse(output.content)
        return result
    except Exception as e:
        # ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        print(f"âš ï¸  LLM-as-Judgeãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
        return _fallback_judge(requirements, evidence_map, matched, gaps, improvements)


def _fallback_judge(
    requirements: List[Requirement],
    evidence_map: Dict[str, Evidence],
    matched: List[RequirementWithEvidence],
    gaps: List[Gap],
    improvements: Improvements
) -> JudgeOutput:
    """
    Fallback: ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§ç°¡æ˜“è©•ä¾¡
    """
    # å¼•ç”¨ã®æ­£ç¢ºæ€§ï¼ˆç°¡æ˜“ãƒã‚§ãƒƒã‚¯ï¼‰
    quote_accuracy = []
    for ev in list(evidence_map.values())[:5]:
        for quote in ev.resume_quotes[:1]:
            quote_accuracy.append(QuoteAccuracyEvaluation(
                req_id=ev.req_id,
                quote=quote[:100],
                is_accurate=True,  # ç°¡æ˜“ç‰ˆã§ã¯Trueã¨ä»®å®š
                accuracy_score=0.8,
                reason="ç°¡æ˜“è©•ä¾¡ã®ãŸã‚è©³ç´°ç¢ºèªãŒå¿…è¦"
            ))
    
    # åˆ†é¡ã®å¦¥å½“æ€§ï¼ˆç°¡æ˜“ãƒã‚§ãƒƒã‚¯ï¼‰
    classification = []
    for req in requirements[:5]:
        classification.append(ClassificationEvaluation(
            req_id=req.req_id,
            current_category=req.category.value,
            is_correct=True,
            correct_category=None,
            confidence_score=0.8,
            reason="ç°¡æ˜“è©•ä¾¡ã®ãŸã‚è©³ç´°ç¢ºèªãŒå¿…è¦"
        ))
    
    # ã‚®ãƒ£ãƒƒãƒ—ã®å¦¥å½“æ€§ï¼ˆç°¡æ˜“ãƒã‚§ãƒƒã‚¯ï¼‰
    gap_validity = []
    for gap in gaps[:5]:
        gap_validity.append(GapEvaluation(
            req_id=gap.requirement.req_id,
            is_gap_correct=True,
            gap_score=0.8,
            reason="ç°¡æ˜“è©•ä¾¡ã®ãŸã‚è©³ç´°ç¢ºèªãŒå¿…è¦"
        ))
    
    # æ”¹å–„æ¡ˆã®å…·ä½“æ€§ï¼ˆç°¡æ˜“ãƒã‚§ãƒƒã‚¯ï¼‰
    improvement_specificity = []
    for edit in improvements.resume_edits[:3]:
        improvement_specificity.append(ImprovementSpecificityEvaluation(
            improvement_type="resume_edit",
            target=edit.target_gap,
            specificity_score=0.8,
            is_specific=True,
            reason="ç°¡æ˜“è©•ä¾¡ã®ãŸã‚è©³ç´°ç¢ºèªãŒå¿…è¦"
        ))
    for item in improvements.action_items[:3]:
        improvement_specificity.append(ImprovementSpecificityEvaluation(
            improvement_type="action_item",
            target=item.action[:50],
            specificity_score=0.8,
            is_specific=True,
            reason="ç°¡æ˜“è©•ä¾¡ã®ãŸã‚è©³ç´°ç¢ºèªãŒå¿…è¦"
        ))
    
    return JudgeOutput(
        quote_accuracy=quote_accuracy,
        classification=classification,
        gap_validity=gap_validity,
        improvement_specificity=improvement_specificity,
        overall_score=0.8,
        overall_feedback="ç°¡æ˜“è©•ä¾¡ãŒå®Ÿè¡Œã•ã‚Œã¾ã—ãŸã€‚LLM-as-Judgeã®è©³ç´°è©•ä¾¡ã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
    )


# ==================== è©•ä¾¡çµæœã®é›†è¨ˆ ====================
def summarize_judge_results(judge_output: JudgeOutput) -> Dict[str, Any]:
    """
    è©•ä¾¡çµæœã‚’é›†è¨ˆã—ã¦ã‚µãƒãƒªãƒ¼ã‚’è¿”ã™
    
    Args:
        judge_output: è©•ä¾¡çµæœ
        
    Returns:
        Dict[str, Any]: é›†è¨ˆçµæœ
    """
    summary = {
        "overall_score": judge_output.overall_score,
        "overall_feedback": judge_output.overall_feedback,
        "quote_accuracy": {
            "average_score": sum(e.accuracy_score for e in judge_output.quote_accuracy) / len(judge_output.quote_accuracy) if judge_output.quote_accuracy else 0.0,
            "total_count": len(judge_output.quote_accuracy),
            "accurate_count": sum(1 for e in judge_output.quote_accuracy if e.is_accurate),
            "issues": [e for e in judge_output.quote_accuracy if not e.is_accurate]
        },
        "classification": {
            "average_score": sum(e.confidence_score for e in judge_output.classification) / len(judge_output.classification) if judge_output.classification else 0.0,
            "total_count": len(judge_output.classification),
            "correct_count": sum(1 for e in judge_output.classification if e.is_correct),
            "issues": [e for e in judge_output.classification if not e.is_correct]
        },
        "gap_validity": {
            "average_score": sum(e.gap_score for e in judge_output.gap_validity) / len(judge_output.gap_validity) if judge_output.gap_validity else 0.0,
            "total_count": len(judge_output.gap_validity),
            "correct_count": sum(1 for e in judge_output.gap_validity if e.is_gap_correct),
            "issues": [e for e in judge_output.gap_validity if not e.is_gap_correct]
        },
        "improvement_specificity": {
            "average_score": sum(e.specificity_score for e in judge_output.improvement_specificity) / len(judge_output.improvement_specificity) if judge_output.improvement_specificity else 0.0,
            "total_count": len(judge_output.improvement_specificity),
            "specific_count": sum(1 for e in judge_output.improvement_specificity if e.is_specific),
            "issues": [e for e in judge_output.improvement_specificity if not e.is_specific]
        }
    }
    
    return summary


# ==================== ãƒ†ã‚¹ãƒˆç”¨ã‚³ãƒ¼ãƒ‰ ====================
if __name__ == "__main__":
    from f1_extract_requirements import extract_requirements
    from f2_extract_evidence import extract_evidence
    from f3_score import calculate_scores
    from f4_generate_improvements import generate_improvements
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    sample_job_text = """
ã€æ±‚äººç¥¨ã€‘Webã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‹Ÿé›†

â– å¿…é ˆã‚¹ã‚­ãƒ«
ãƒ»Pythoné–‹ç™ºçµŒé¨“3å¹´ä»¥ä¸Š
ãƒ»Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºã®å®Ÿå‹™çµŒé¨“

â– æ­“è¿ã‚¹ã‚­ãƒ«
ãƒ»AWSãªã©ã‚¯ãƒ©ã‚¦ãƒ‰ç’°å¢ƒã§ã®é–‹ç™ºçµŒé¨“
    """
    
    sample_resume_text = """
ã€è·å‹™çµŒæ­´æ›¸ã€‘

â– è·å‹™çµŒæ­´
2019å¹´ã€œç¾åœ¨ï¼šæ ªå¼ä¼šç¤¾ABC
ãƒ»Pythonã‚’ä½¿ç”¨ã—ãŸWebã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºã«5å¹´é–“å¾“äº‹
ãƒ»Djangoãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ç”¨ã„ãŸECã‚µã‚¤ãƒˆã®æ§‹ç¯‰
ãƒ»AWS (EC2, S3, RDS) ã‚’æ´»ç”¨ã—ãŸã‚¤ãƒ³ãƒ•ãƒ©æ§‹ç¯‰
    """
    
    print("=" * 60)
    print("LLM-as-Judge ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    try:
        # F1ã€œF4ã‚’å®Ÿè¡Œ
        requirements = extract_requirements(sample_job_text)
        evidence_map = extract_evidence(sample_resume_text, requirements)
        score_total, score_must, score_want, matched, gaps, summary = calculate_scores(requirements, evidence_map)
        improvements = generate_improvements(sample_job_text, sample_resume_text, requirements, matched, gaps)
        
        # LLM-as-Judgeã§è©•ä¾¡
        print("\n[å®Ÿè¡Œ] LLM-as-Judgeè©•ä¾¡")
        judge_output = evaluate_with_llm_judge(
            sample_job_text,
            sample_resume_text,
            requirements,
            evidence_map,
            matched,
            gaps,
            improvements
        )
        
        # çµæœè¡¨ç¤º
        print(f"\n{'='*60}")
        print("ğŸ“Š è©•ä¾¡çµæœ")
        print(f"{'='*60}")
        print(f"ç·åˆã‚¹ã‚³ã‚¢: {judge_output.overall_score:.2f}")
        print(f"\nã€ç·åˆãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã€‘\n{judge_output.overall_feedback}")
        
        summary = summarize_judge_results(judge_output)
        print(f"\nã€å¼•ç”¨ã®æ­£ç¢ºæ€§ã€‘")
        print(f"  å¹³å‡ã‚¹ã‚³ã‚¢: {summary['quote_accuracy']['average_score']:.2f}")
        print(f"  æ­£ç¢º: {summary['quote_accuracy']['accurate_count']}/{summary['quote_accuracy']['total_count']}")
        
        print(f"\nã€åˆ†é¡ã®å¦¥å½“æ€§ã€‘")
        print(f"  å¹³å‡ã‚¹ã‚³ã‚¢: {summary['classification']['average_score']:.2f}")
        print(f"  æ­£ã—ã„: {summary['classification']['correct_count']}/{summary['classification']['total_count']}")
        
        print(f"\nã€ã‚®ãƒ£ãƒƒãƒ—ã®å¦¥å½“æ€§ã€‘")
        print(f"  å¹³å‡ã‚¹ã‚³ã‚¢: {summary['gap_validity']['average_score']:.2f}")
        print(f"  æ­£ã—ã„: {summary['gap_validity']['correct_count']}/{summary['gap_validity']['total_count']}")
        
        print(f"\nã€æ”¹å–„æ¡ˆã®å…·ä½“æ€§ã€‘")
        print(f"  å¹³å‡ã‚¹ã‚³ã‚¢: {summary['improvement_specificity']['average_score']:.2f}")
        print(f"  å…·ä½“çš„: {summary['improvement_specificity']['specific_count']}/{summary['improvement_specificity']['total_count']}")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

